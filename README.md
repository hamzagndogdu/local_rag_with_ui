#  RAG Conversational Chatbot â€” FastAPI + Ollama + ChromaDB + Next.js

This project is a fully local **Retrieval-Augmented Generation (RAG)** chatbot system built using:

- **Ollama** â†’ LLM inference (Gemma 3:4B)
- **ChromaDB** â†’ Vector database for document retrieval
- **FastAPI** â†’ Backend API for RAG pipeline
- **Next.js** â†’ Frontend Chat UI
- **Python (venv)** â†’ Embedding generation & preprocessing

The system allows you to upload PDF documents, embed them, store them in ChromaDB, and query them via a custom chat UI.

---

# âš™ï¸ System Architecture

User â†’ Next.js UI (Port 3000)
â†“
FastAPI Backend (Port 8000)
â†“
RAG Pipeline (ChromaDB + Embeddings)
â†“
Ollama (LLM Server, Port 11434)



---

# ğŸš€ Setup Instructions

Follow the steps below to set up and run the entire pipeline.

---

## âœ” Step 0 â€” Install Ollama (Required)

Download Ollama:

ğŸ‘‰ https://ollama.com/download

Pull required models:

```bash
ollama pull gemma3:4b
ollama pull mxbai-embed-large:latest
```


## Step 1 â€” Create and Activate Python Virtual Environment
```bash
python -m venv venv
```

Install dependencies:
```bash
pip install -r requirements.txt
```

## Step 2 â€” Start the Ollama Model Server In a New Terminal
```bash
ollama serve
```

## Step 3 â€” Generate Embeddings (Vectorization)
```bash
python vectorize.py
```

## Step 4 â€” Start the FastAPI Backend (RAG API)

```bash
cd rag_backend
```

Run FastAPI:
```bash
uvicorn main:app --reload --port 8000
```

Step 5 â€” Start the Next.js Chat UI (Frontend)
Inside rag_ui/:

```bash
npm install
npm run dev
```
