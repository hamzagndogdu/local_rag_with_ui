#  RAG Conversational Chatbot â€” FastAPI + Ollama + ChromaDB + Next.js

This project is a fully local **Retrieval-Augmented Generation (RAG)** chatbot system built using:

- **Ollama** â†’ LLM inference (Gemma 3:4B)
- **ChromaDB** â†’ Vector database for document retrieval
- **FastAPI** â†’ Backend API for RAG pipeline
- **Next.js** â†’ Frontend Chat UI
- **Python (venv)** â†’ Embedding generation & preprocessing

The system allows you to upload PDF documents, embed them, store them in ChromaDB, and query them via a custom chat UI.

---
## Q_A
<img src="images/Q_A.jpg" alt="Chat UI Screenshot" width="600">



## RESOURCE (DnD_BasicRules_2018)
<img src="images/D&D PDF.jpg" alt="PDF Source Example" width="600">





# âš™ï¸ System Architecture

User â†’ Next.js UI (Port 3000)
â†“
FastAPI Backend (Port 8000)
â†“
RAG Pipeline (ChromaDB + Embeddings)
â†“
Ollama (LLM Server, Port 11434)



---

# ğŸš€ Setup Instructions

Follow the steps below to set up and run the entire pipeline.

---

## âœ” Step 0 â€” Install Ollama (Required)

Download Ollama:

ğŸ‘‰ https://ollama.com/download

Pull required models (Or choose other models) :

```bash
ollama pull gemma3:4b
```
```bash
ollama pull mxbai-embed-large:latest
```


## âœ” Step 1 â€” Create and Activate Python Virtual Environment
```bash
python -m venv venv
```

```bash
./venv/Scripts/activate
```

Install dependencies:
```bash
pip install -r requirements.txt
```

## âœ” Step 2 â€” Start the Ollama Model Server In a New Terminal
```bash
ollama serve
```

## âœ” Step 3 â€” Generate Embeddings (Vectorization)
```bash
python vectorize.py
```

## âœ” Step 4 â€” Start the FastAPI Backend (RAG API)

```bash
cd rag_backend
```

Run FastAPI:
```bash
uvicorn main:app --reload --port 8000
```

## âœ” Step 5 â€” Start the Next.js Chat UI (Frontend)
Inside rag_ui/:

```bash
npm install
```
```bash
npm run dev
```


[![Open UI](https://img.shields.io/badge/Open%20UI-localhost%3A3000-blue)](http://localhost:3000)
